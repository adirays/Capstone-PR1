{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline\n",
    "\n",
    "**Project**: Manufacturing Output Prediction  \n",
    "**Description**: Standardized preprocessing pipeline for Day-1 manufacturing data.  \n",
    "**Input**: `data/raw/manufacturing_dataset_1000_samples.csv`  \n",
    "**Output**: `data/interim/cleaned_data.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# =========================================\n",
    "# CONSTANTS & CONFIGURATION\n",
    "# =========================================\n",
    "RAW_DATA_PATH = '../data/raw/manufacturing_dataset_1000_samples.csv'\n",
    "OUTPUT_PATH = '../data/interim/cleaned_data.csv'\n",
    "OUTPUT_DIR = os.path.dirname(OUTPUT_PATH)\n",
    "\n",
    "TARGET_COL = 'Parts_Per_Hour'\n",
    "\n",
    "CAT_COLS = [\n",
    "    'Shift',\n",
    "    'Machine_Type',\n",
    "    'Material_Grade',\n",
    "    'Day_of_Week'\n",
    "]\n",
    "\n",
    "NUM_COLS = [\n",
    "    'Injection_Temperature',\n",
    "    'Injection_Pressure',\n",
    "    'Cycle_Time',\n",
    "    'Cooling_Time',\n",
    "    'Material_Viscosity',\n",
    "    'Ambient_Temperature',\n",
    "    'Machine_Age',\n",
    "    'Operator_Experience',\n",
    "    'Maintenance_Hours',\n",
    "    'Machine_Utilization',\n",
    "    'Temperature_Pressure_Ratio',\n",
    "    'Total_Cycle_Time',\n",
    "    'Efficiency_Score'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading data...\")\n",
    "if not os.path.exists(RAW_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Input file not found at {RAW_DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(RAW_DATA_PATH)\n",
    "\n",
    "initial_shape = df.shape\n",
    "print(f\"Initial Data Shape: {initial_shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Column Management and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly drop Timestamp if it exists\n",
    "if 'Timestamp' in df.columns:\n",
    "    print(\"Dropping 'Timestamp' column...\")\n",
    "    df = df.drop(columns=['Timestamp'])\n",
    "else:\n",
    "    print(\"'Timestamp' column not found, skipping drop.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Handling missing values...\")\n",
    "# Identify actual numerical and categorical columns present in the dataframe\n",
    "# (Intersection with defined constants to be safe)\n",
    "present_num_cols = [c for c in NUM_COLS if c in df.columns]\n",
    "present_cat_cols = [c for c in CAT_COLS if c in df.columns]\n",
    "\n",
    "# Impute Numerical with Median\n",
    "for col in present_num_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "\n",
    "# Impute Categorical with Mode\n",
    "for col in present_cat_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "# Brief assertion to ensure no NaNs in feature columns\n",
    "assert df[present_num_cols + present_cat_cols].isnull().sum().sum() == 0, \"Missing values remain after imputation!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Encoding (One-Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoding categorical features...\")\n",
    "# One-hot encode using pandas get_dummies\n",
    "df_encoded = pd.get_dummies(df, columns=present_cat_cols, dtype=int)\n",
    "\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaling numerical features...\")\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling only to the numerical columns defined\n",
    "df_encoded[present_num_cols] = scaler.fit_transform(df_encoded[present_num_cols])\n",
    "\n",
    "# Check stats of a scaled column (mean should be approx 0, std approx 1)\n",
    "if present_num_cols:\n",
    "    sample_col = present_num_cols[0]\n",
    "    print(f\"Stats for {sample_col} after scaling: Mean={df_encoded[sample_col].mean():.4f}, Std={df_encoded[sample_col].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validating processed data...\")\n",
    "\n",
    "# 1. No missing values\n",
    "assert df_encoded.isnull().sum().sum() == 0, \"Final dataset contains missing values!\"\n",
    "\n",
    "# 2. Target column preserved\n",
    "assert TARGET_COL in df_encoded.columns, f\"Target column '{TARGET_COL}' is missing!\"\n",
    "\n",
    "# 3. Row count unchanged\n",
    "assert df_encoded.shape[0] == initial_shape[0], f\"Row count changed! Initial: {initial_shape[0]}, Final: {df_encoded.shape[0]}\"\n",
    "\n",
    "# 4. All features numeric\n",
    "# Check if all dtypes are numeric (int or float) - Exclude object type\n",
    "non_numeric_cols = df_encoded.select_dtypes(include=['object']).columns\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(f\"Warning: Non-numeric columns found: {non_numeric_cols}\")\n",
    "assert len(non_numeric_cols) == 0, \"Final dataset contains non-numeric columns!\"\n",
    "\n",
    "print(\"All validations passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Saving processed data to {OUTPUT_PATH}...\")\n",
    "df_encoded.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"Pipeline complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
